{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "%pip install -q petals"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-G8wCluUgFbe",
        "outputId": "992957e4-7532-4dbd-c3e0-b76c16f0e1a3"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.3/92.3 KB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.1/9.1 MB\u001b[0m \u001b[31m42.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.9/55.9 MB\u001b[0m \u001b[31m12.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.8/5.8 MB\u001b[0m \u001b[31m30.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m182.4/182.4 KB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 KB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m191.5/191.5 KB\u001b[0m \u001b[31m12.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.2/4.2 MB\u001b[0m \u001b[31m47.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.6/4.6 MB\u001b[0m \u001b[31m39.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m13.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.6/7.6 MB\u001b[0m \u001b[31m79.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m54.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m65.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m46.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m33.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m63.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for varint (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KanSpJiNlSIb",
        "outputId": "e32d7ab7-c685-407b-fa6f-d8896cc03b17"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "tnXAYFQbfeW6"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "import json\n",
        "import torch\n",
        "from tqdm import tqdm, trange\n",
        "from transformers import BloomTokenizerFast\n",
        "from petals import DistributedBloomForCausalLM\n",
        "from random import shuffle\n",
        "import sys"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/openai/grade-school-math.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a31w00skgOVM",
        "outputId": "b7810920-f5c5-4d6d-8292-dc5b32978a74"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'grade-school-math'...\n",
            "remote: Enumerating objects: 36, done.\u001b[K\n",
            "remote: Counting objects: 100% (20/20), done.\u001b[K\n",
            "remote: Compressing objects: 100% (13/13), done.\u001b[K\n",
            "remote: Total 36 (delta 10), reused 7 (delta 7), pack-reused 16\u001b[K\n",
            "Unpacking objects: 100% (36/36), 3.13 MiB | 3.68 MiB/s, done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "PROMPT_FILE_PATH = '/content/drive/MyDrive/socratic_CoT_prompt'\n",
        "RESULTS_FILE_PATH = '/content/drive/MyDrive/output_self-consistency_method'\n",
        "ITERATIONS_COUNT = 8\n",
        "PARAMS = {\n",
        "    \"do_sample\": True,\n",
        "    \"temperature\": 0.7,\n",
        "    \"top_k\": 40,\n",
        "    \"top_p\": None,\n",
        "    \"num_beams\": 1,\n",
        "    \"max_new_tokens\": 100,\n",
        "    \"num_return_sequences\": None,\n",
        "    \"stop\": [\"Q:\"]\n",
        "}\n",
        "TASK_COUNT = 100"
      ],
      "metadata": {
        "id": "cQxXfUKQf8k-"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def make_prompt(cot_prompt, cot_answers):\n",
        "    prompt = ''\n",
        "    for example, answer in zip(cot_prompt, cot_answers):\n",
        "        prompt += 'Q: ' + example['question'] + '\\nA: ' + example['answer'] + 'The answer is ' + str(answer) + '\\n\\n'\n",
        "    with open(PROMPT_FILE_PATH, 'w') as prompt_file:\n",
        "        prompt_file.write(prompt)\n",
        "    return prompt"
      ],
      "metadata": {
        "id": "ejsf2DgtgBJN"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_input_and_prompt(test_path, prompt):\n",
        "    with open(test_path, 'r') as data_file:\n",
        "        test_problems = data_file.readlines()\n",
        "    # extract final answer\n",
        "    answer_list = [float(re.search(r'#### ([0-9-]+)', problem).group(1)) for problem in test_problems]\n",
        "    # remove annotations of calculator and final answer, convert string to dict\n",
        "    input_list = [json.loads(re.sub('<<[0-9\\(\\)\\.=+*/-]*>>|#### ([0-9-]+)', '', problem))\n",
        "                  for problem in test_problems]\n",
        "    combined = list(zip(answer_list, input_list))\n",
        "    shuffle(combined)\n",
        "    answer_list[:], input_list[:] = zip(*combined)\n",
        "    if prompt is not None:\n",
        "        cot_prompt = prompt\n",
        "    else:\n",
        "        cot_prompt = make_prompt(input_list[:11], answer_list[:11])\n",
        "        answer_list = answer_list[11:]\n",
        "        input_list = input_list[11:]\n",
        "    return input_list, answer_list, cot_prompt"
      ],
      "metadata": {
        "id": "7kJggvQngCOl"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == '__main__':\n",
        "    results = {\n",
        "            \"Parameters_of_generation\": PARAMS,\n",
        "            \"Outputs\": []\n",
        "    }\n",
        "    if RESULTS_FILE_PATH is not None:\n",
        "        results = json.load(open(RESULTS_FILE_PATH, 'r'))\n",
        "    with open(PROMPT_FILE_PATH, 'r') as prompt_file:\n",
        "        prompt = prompt_file.read()\n",
        "        start_idx = len(results[\"Outputs\"])\n",
        "    test_path = './grade-school-math/grade_school_math/data/test.jsonl'\n",
        "    input_list, answer_list, cot_prompt = get_input_and_prompt(test_path, prompt)\n",
        "    \n",
        "    MODEL_NAME = \"bigscience/bloomz-petals\"\n",
        "    tokenizer = BloomTokenizerFast.from_pretrained(MODEL_NAME)\n",
        "    model = DistributedBloomForCausalLM.from_pretrained(MODEL_NAME, request_timeout=300, daemon_startup_timeout=120)\n",
        "    model = model.cuda()\n",
        "    for input, answer in tqdm(zip(input_list[start_idx:TASK_COUNT], answer_list[start_idx:TASK_COUNT])):\n",
        "        task_data = {\n",
        "            \"Question\" : input['question'],\n",
        "            \"Answer\": str(answer),\n",
        "            \"BLOOM_answers\": []\n",
        "        }\n",
        "        for i in trange(ITERATIONS_COUNT):\n",
        "            tokenized_input = tokenizer(cot_prompt + 'Q: ' + input['question'] + '\\nA: ', return_tensors=\"pt\")[\n",
        "            \"input_ids\"].cuda()\n",
        "            output = model.generate(tokenized_input, **PARAMS)\n",
        "            result = {}\n",
        "            shift = len(cot_prompt) + len(input['question']) + 3\n",
        "            task_data[\"BLOOM_answers\"].append(tokenizer.decode(output[0])[shift:])\n",
        "        results[\"Outputs\"].append(task_data)\n",
        "        with open(RESULTS_FILE_PATH, 'w') as output_file:\n",
        "            json.dump(results, output_file)"
      ],
      "metadata": {
        "id": "_wc5gNjNgQJ1"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}