## Тестовое задаание в Tinkoff NLP lab

1. BLOOM-176B - это большая языковая модель, имеющая 176 миллиардов параметров. С помощью распределённой системы Petals появляется возможность использования данной нейросети без наличия у конкретного клиента огромных вычислительных ресурсов, необходимых для её работы. Так как работа системы достаточно нестабильна из-за отключения/подключения узлов, то в данном исследовании было принято решение запустить арендованные сервера с GPU для повышения ёмкости системы Petals. Для данной цели было арендовано два сервера с Nvidia Tesla A10 и предустановленными драйверами для CUDA (https://immers.cloud/vm/create/?FlavorID=528)
![image](https://user-images.githubusercontent.com/62150448/220173777-c563adea-53ee-4173-b7a7-ea640a85ea1b.png)

Скрипт server_run.sh содержит команды необходимые для создания необходимого окружения и запуска контейнера Docker для подключения узла к Petals.

2. Для проведения экспериментов был также арендован сервер с аналогичными характеристиками для клиента. Скрипт client_run.sh содержит команды для установки необходимого окружения на клиенте.

3. Датасет GSM8K состоит из 8,5 тыс. высококачественных школьных математических задач, созданных людьми, составляющими задачи. Они разделены на 7,5 тыс. обучающих задач и 1 тыс. тестовых задач. Для решения этих задач требуется от 2 до 8 шагов, и решения в основном включают выполнение последовательности элементарных вычислений с использованием основных арифметических операций (+ - / *) для получения окончательного ответа. Файлы с тренировочными и тестовыми данными:
* grade_school_math/data/train.jsonl
* grade_school_math/data/test.jsonl

Каждая строка этих файлов соответствует одной математической задаче начальной школы, сохраненной в виде словаря json (с ключом «question» и ключом «answer»). Ответ отформатирован таким образом, что в нем используются аннотации вычислений, а окончательное числовое решение является последней строкой решения, которому предшествует ####. В данном исследовании используется не весь датасет, а лишь часть его тестовой выборки.

4. Файлы Chain_of_thoughts_method_with_BLOOM_176B.ipynb и Self_consistency_method.ipynb нужны для генерации результатов к задачам из датасета GSM8K с помощью двух исследуемых методов: CoT, Self-consistency method. Сгенерированные результаты хранятся в директории method_results, а используемые ими файлы подсказок находятся в директории prompts.

5. Ноутбук Calculate_metrics.ipynb содержит подсчёт метрик по итоговым ответам на математические задачи, а также содержит анализ результатов работы исследуемых методов и наиболее частые проблемы.

6. Директория prompt_tuning содержит наработки по улучшению качества работы BLOOM-176B на датасете GSM8K с помощью метода prompt tuning. В частности там есть скрипт для дообучения "trainable continuous embeddings" добавляемых к модели и скрипт по использованию дообученной модели для генерации ответов на задачи. Скрипт model_tuning_preprocessing.sh необходим для создания окружения на машине, на которой запускается обучение и инференс модели.
