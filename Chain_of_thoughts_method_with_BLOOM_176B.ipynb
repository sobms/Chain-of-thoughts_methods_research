{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "%pip install -q petals"
      ],
      "metadata": {
        "id": "4HqrT6wVg1xF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "464bd29e-149c-4b56-cdce-5a5d35f583bd"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.3/92.3 KB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m182.4/182.4 KB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.8/5.8 MB\u001b[0m \u001b[31m40.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.1/9.1 MB\u001b[0m \u001b[31m32.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 KB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m191.5/191.5 KB\u001b[0m \u001b[31m732.8 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.9/55.9 MB\u001b[0m \u001b[31m12.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.6/4.6 MB\u001b[0m \u001b[31m73.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.2/4.2 MB\u001b[0m \u001b[31m26.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m23.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.6/7.6 MB\u001b[0m \u001b[31m44.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m52.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m93.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m99.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m96.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m90.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for varint (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import json\n",
        "import torch\n",
        "from tqdm import tqdm\n",
        "from transformers import BloomTokenizerFast \n",
        "from petals import DistributedBloomForCausalLM\n",
        "from random import shuffle"
      ],
      "metadata": {
        "id": "lu-NIMyraEqC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n9rDr-PYKoOf"
      },
      "outputs": [],
      "source": [
        "!git clone https://github.com/openai/grade-school-math.git"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "9AE6mQxwBI77"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## GSM8K. "
      ],
      "metadata": {
        "id": "wiaOsWMWXeXY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "GSM8K состоит из 8,5 тыс. высококачественных школьных математических задач, созданных людьми, составляющими задачи. Они разделены на 7,5 тыс. обучающих задач и 1 тыс. тестовых задач. Для решения этих задач требуется от 2 до 8 шагов, и решения в основном включают выполнение последовательности элементарных вычислений с использованием основных арифметических операций (+ - / *) для получения окончательного ответа."
      ],
      "metadata": {
        "id": "hjBM_FiAXjPn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Файлы с тренировочными и тестовыми данными:\n",
        "*   grade_school_math/data/train.jsonl\n",
        "*   grade_school_math/data/test.jsonl\n",
        "\n",
        "Каждая строка этих файлов соответствует одной математической задаче начальной школы, сохраненной в виде словаря json (с ключом «question» и ключом «answer»). Ответ отформатирован таким образом, что в нем используются аннотации вычислений, а окончательное числовое решение является последней строкой решения, которому предшествует ####"
      ],
      "metadata": {
        "id": "ClYqnWQ3X4VW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_path = './grade-school-math/grade_school_math/data/train.jsonl'\n",
        "test_path = './grade-school-math/grade_school_math/data/test.jsonl'"
      ],
      "metadata": {
        "id": "Xu24xVXxWT_f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Чтобы извлечь окончательное числовое решение для определенного вопроса, просто проанализируйте конец ответа, чтобы извлечь числовое значение сразу после токена ####."
      ],
      "metadata": {
        "id": "EG4Hw-YFcFzB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Для удаления аннотаций калькулятора, необходимо удалить строки, начинающиеся с << и заканчивающиеся на >>."
      ],
      "metadata": {
        "id": "j74h7GvCbJm6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def make_prompt(cot_prompt, cot_labels):\n",
        "    prompt = ''\n",
        "    for example, label in zip(cot_prompt, cot_labels):\n",
        "        prompt += 'Q: ' + example['question'] + '\\nA: ' + example['answer'] + 'The answer is ' + str(label) + '\\n\\n'          \n",
        "    return prompt"
      ],
      "metadata": {
        "id": "9dGbJSRvcmw8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_input_and_prompt(test_path, prompt_file_path = None):\n",
        "    with open(test_path, 'r') as data_file:\n",
        "        test_problems = data_file.readlines()\n",
        "    # extract final answer\n",
        "    answer_list = [float(re.search(r'#### ([0-9-]+)', problem).group(1)) for problem in test_problems]\n",
        "    # remove annotations of calculator and final answer, convert string to dict\n",
        "    input_list = [json.loads(re.sub('<<[0-9\\(\\)\\.=+*/-]*>>|#### ([0-9-]+)', '', problem))\n",
        "                  for problem in test_problems]\n",
        "    combined = list(zip(answer_list, input_list))\n",
        "    shuffle(combined)\n",
        "    answer_list[:], input_list[:] = zip(*combined)\n",
        "    if prompt_file_path is not None:\n",
        "      with open(prompt_file_path, 'r') as prompt_file:\n",
        "        prompt = prompt_file.read()\n",
        "        cot_prompt = prompt\n",
        "    else:\n",
        "        cot_prompt = make_prompt(input_list[:11], answer_list[:11])\n",
        "        answer_list = answer_list[11:]\n",
        "        input_list = input_list[11:]\n",
        "    return input_list, answer_list, cot_prompt"
      ],
      "metadata": {
        "id": "Uu_uVpKmUro2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## BLOOM-176B"
      ],
      "metadata": {
        "id": "iDFtFMttgqxZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "MODEL_NAME = \"bigscience/bloomz-petals\"\n",
        "tokenizer = BloomTokenizerFast.from_pretrained(MODEL_NAME)\n",
        "model = DistributedBloomForCausalLM.from_pretrained(MODEL_NAME, request_timeout=300, daemon_startup_timeout=120)\n",
        "model = model.cuda()"
      ],
      "metadata": {
        "id": "JFosXvvPgvcp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Chain-of-thoughts method with BLOOM-176B"
      ],
      "metadata": {
        "id": "1_9RfzxxkyeF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Firstly, let's make chain-of-thougths prompts and input questions for bloom"
      ],
      "metadata": {
        "id": "aoVDEEvLjNVk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "PROMPT_FILE_PATH = '/content/socratic_CoT_prompt'\n",
        "RESULTS_FILE_PATH = '/content/drive/MyDrive/output_CoT_method'"
      ],
      "metadata": {
        "id": "GN0HScT-VIrF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_list, answer_list, cot_prompt = get_input_and_prompt(test_path, PROMPT_FILE_PATH)"
      ],
      "metadata": {
        "id": "VkEYvg1L56ou"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "TASK_COUNT = len(input_list)\n",
        "PARAMS = {\n",
        "    \"do_sample\": None,\n",
        "    \"temperature\": 1.0,\n",
        "    \"top_k\": None,\n",
        "    \"top_p\": None,\n",
        "    \"num_beams\": 1,\n",
        "    \"max_new_tokens\": 100,\n",
        "    \"num_return_sequences\": None,\n",
        "    \"stop\": [\"Q:\"]\n",
        "}"
      ],
      "metadata": {
        "id": "ytNcv1tnVsuo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "results = {\n",
        "        \"Parameters_of_generation\": PARAMS,\n",
        "        \"Outputs\": []\n",
        "}\n",
        "if RESULTS_FILE_PATH is not None:\n",
        "    results = json.load(open(RESULTS_FILE_PATH, 'r'))\n",
        "    TASK_COUNT = len(input_list)\n",
        "    start_idx = len(results[\"Outputs\"])\n",
        "for input, answer in tqdm(zip(input_list[start_idx:TASK_COUNT], answer_list[start_idx:TASK_COUNT])):\n",
        "    task_data = {\n",
        "        \"Question\" : input['question'],\n",
        "        \"Answer\": str(answer),\n",
        "        \"BLOOM_answer\": None\n",
        "    }\n",
        "    \n",
        "    tokenized_input = tokenizer(cot_prompt + 'Q: ' + input['question'] + '\\nA: ', return_tensors=\"pt\")[\"input_ids\"].cuda()\n",
        "    output = model.generate(tokenized_input, **PARAMS)\n",
        "    shift = len(cot_prompt) + len(input['question']) + 3\n",
        "    task_data[\"BLOOM_answer\"] = tokenizer.decode(output[0])[shift:]\n",
        "    results[\"Outputs\"].append(task_data)\n",
        "    with open(RESULTS_FILE_PATH, 'w') as output_file:\n",
        "         json.dump(results, output_file)"
      ],
      "metadata": {
        "id": "ahjmVIANk8Vc"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}